{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4c3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a9ef0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channelName</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>views</th>\n",
       "      <th>totalVideos</th>\n",
       "      <th>playlistId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>6330000</td>\n",
       "      <td>493771675</td>\n",
       "      <td>1081</td>\n",
       "      <td>UUoOae5nYA7VqaXzerajD0lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channelName  subscribers      views  totalVideos                playlistId\n",
       "0  Ali Abdaal      6330000  493771675         1081  UUoOae5nYA7VqaXzerajD0lg"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(r\"C:\\Users\\hafss\\OneDrive\\Desktop\\PFA_2A\\youtube\\data\\channel_stats.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185f2ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QF2wIywvDhk</td>\n",
       "      <td>['Hope you enjoyed the video!! Get the list of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-r2SH6EH5eY</td>\n",
       "      <td>['Hope you enjoyed the video!! If you fancy ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmCaQxk4b8c</td>\n",
       "      <td>['Hope you enjoyed the video!! If you fancy ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k_K9MbqNhA0</td>\n",
       "      <td>['Hope you enjoyed the video!! If you fancy ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X_XNSeW9jok</td>\n",
       "      <td>['LifeNotes: Hope you enjoyed the video!! If y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>FEUIxnnW5-w</td>\n",
       "      <td>['Beautiful voice, so dreamy! üòç', 'üíú', 'ÿßŸÑŸÖŸàÿ≥Ÿä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>d2sMEE1NrFc</td>\n",
       "      <td>['Adorable ‚ù§', 'Aeee :)\\nWho‚Äôs watching in 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>7APfFjfnNBc</td>\n",
       "      <td>['buetifull voice!', 'She has such a gentle vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>XJHM0fEH3ss</td>\n",
       "      <td>['bruh, i thougt you were the singer but witho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>LQ3Mu8A7gjY</td>\n",
       "      <td>['wathing every video from beginning to end', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1077 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                           comments\n",
       "0     QF2wIywvDhk  ['Hope you enjoyed the video!! Get the list of...\n",
       "1     -r2SH6EH5eY  ['Hope you enjoyed the video!! If you fancy ge...\n",
       "2     lmCaQxk4b8c  ['Hope you enjoyed the video!! If you fancy ge...\n",
       "3     k_K9MbqNhA0  ['Hope you enjoyed the video!! If you fancy ge...\n",
       "4     X_XNSeW9jok  ['LifeNotes: Hope you enjoyed the video!! If y...\n",
       "...           ...                                                ...\n",
       "1072  FEUIxnnW5-w  ['Beautiful voice, so dreamy! üòç', 'üíú', 'ÿßŸÑŸÖŸàÿ≥Ÿä...\n",
       "1073  d2sMEE1NrFc  ['Adorable ‚ù§', 'Aeee :)\\nWho‚Äôs watching in 202...\n",
       "1074  7APfFjfnNBc  ['buetifull voice!', 'She has such a gentle vo...\n",
       "1075  XJHM0fEH3ss  ['bruh, i thougt you were the singer but witho...\n",
       "1076  LQ3Mu8A7gjY  ['wathing every video from beginning to end', ...\n",
       "\n",
       "[1077 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv(r\"C:\\Users\\hafss\\OneDrive\\Desktop\\PFA_2A\\youtube\\data\\comments.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce61a35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# The error occurs during this import:\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:533\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    535\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "# The code itself is syntactically correct.\n",
    "# The NameError arises from an issue with the PyTorch installation,\n",
    "# not from a mistake in this script file.\n",
    "# Please fix your PyTorch installation using the official guide.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# The error occurs during this import:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class YouTubeCommentsProcessor:\n",
    "    \"\"\"Advanced processor for YouTube comments dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path=None, df=None):\n",
    "        if df is not None:\n",
    "            self.df = df\n",
    "        elif csv_path is not None:\n",
    "             # Added check for csv_path to avoid error if both are None\n",
    "             self.df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "             raise ValueError(\"Either csv_path or df must be provided.\")\n",
    "\n",
    "        self.processed_comments = []\n",
    "        self.comment_features = []\n",
    "\n",
    "    def parse_comments_list(self, comments_str):\n",
    "        \"\"\"Parse the stringified list of comments\"\"\"\n",
    "        try:\n",
    "            # Handle the string representation of list\n",
    "            comments_list = ast.literal_eval(comments_str)\n",
    "            return comments_list if isinstance(comments_list, list) else [comments_str]\n",
    "        except:\n",
    "            # Fallback for malformed strings or non-list strings\n",
    "            # Ensure it returns a list of strings\n",
    "            if isinstance(comments_str, str):\n",
    "                return [comments_str]\n",
    "            else:\n",
    "                 return [] # Return empty list for non-string input\n",
    "\n",
    "\n",
    "    def extract_comment_features(self, comment):\n",
    "        \"\"\"Extract rich features from individual comments\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Basic text features\n",
    "        features['length'] = len(comment)\n",
    "        features['word_count'] = len(comment.split())\n",
    "        features['has_url'] = 1 if any(url in comment.lower() for url in ['http', 'www.', '.com']) else 0\n",
    "        features['has_mention'] = 1 if '@' in comment else 0\n",
    "        # More robust emoji check (covers wider range)\n",
    "        features['has_emoji'] = 1 if re.search(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U00002639]', comment) else 0\n",
    "        features['exclamation_count'] = comment.count('!')\n",
    "        features['question_count'] = comment.count('?')\n",
    "        features['caps_ratio'] = sum(1 for c in comment if c.isupper()) / len(comment) if len(comment) > 0 else 0\n",
    "\n",
    "        # Sentiment indicators (can be expanded)\n",
    "        positive_words = ['great', 'amazing', 'love', 'awesome', 'thank', 'good', 'best', 'excellent']\n",
    "        negative_words = ['bad', 'hate', 'terrible', 'worst', 'awful', 'disappointed', 'boring']\n",
    "\n",
    "        features['positive_word_count'] = sum(1 for word in positive_words if word in comment.lower())\n",
    "        features['negative_word_count'] = sum(1 for word in negative_words if word in comment.lower())\n",
    "\n",
    "        # Topic indicators based on your data (can be expanded)\n",
    "        productivity_keywords = ['productivity', 'focus', 'work', 'time', 'efficient', 'procrastinate']\n",
    "        finance_keywords = ['money', 'rich', 'wealth', 'invest', 'debt', 'financial', 'million']\n",
    "        motivation_keywords = ['inspire', 'motivate', 'dream', 'goal', 'success', 'achieve']\n",
    "\n",
    "        features['productivity_score'] = sum(1 for word in productivity_keywords if word in comment.lower())\n",
    "        features['finance_score'] = sum(1 for word in finance_keywords if word in comment.lower())\n",
    "        features['motivation_score'] = sum(1 for word in motivation_keywords if word in comment.lower())\n",
    "\n",
    "        return features\n",
    "\n",
    "    def classify_comment_type(self, comment):\n",
    "        \"\"\"Classify comments into categories based on content analysis\"\"\"\n",
    "        comment_lower = comment.lower()\n",
    "\n",
    "        # Define classification rules based on your actual data patterns\n",
    "        # Ensure 'black algorithm' is split or checked carefully if it appears as one token\n",
    "        if any(spam_indicator in comment_lower for spam_indicator in ['black algorithm', 'betterhelp', 'lois e wilson', 'check out my channel', 'subscribe', 'link in bio']): # Added more spam indicators\n",
    "            return 'spam_promotional'\n",
    "        elif any(gratitude in comment_lower for gratitude in ['thank', 'grateful', 'appreciate', 'thanks']): # Added 'thanks'\n",
    "            return 'gratitude'\n",
    "        elif any(question in comment for question in ['?', 'how', 'what', 'where', 'when', 'why', 'can you', 'could you']): # Added more question starters\n",
    "            return 'question'\n",
    "        elif any(criticism in comment_lower for criticism in ['disappointed', 'lazy', 'disagree', 'wrong', 'bad', 'terrible', 'hate']): # Added more criticism terms\n",
    "            return 'criticism'\n",
    "        elif any(personal in comment_lower for personal in ['i am', 'my', 'personally', 'i have', 'i feel', 'in my opinion', 'my experience']): # Added more personal indicators\n",
    "            return 'personal_story'\n",
    "        elif len(comment) > 200: # Increased threshold for detailed feedback\n",
    "            return 'detailed_feedback'\n",
    "        else:\n",
    "            return 'general_comment'\n",
    "\n",
    "    def process_dataset(self):\n",
    "        \"\"\"Process the entire dataset and extract features\"\"\"\n",
    "        all_comments = []\n",
    "        # video_comment_mapping = [] # This variable is created but not used\n",
    "\n",
    "        # Check if df is loaded\n",
    "        if self.df is None or self.df.empty:\n",
    "             print(\"DataFrame is empty or not loaded. Cannot process.\")\n",
    "             self.processed_df = pd.DataFrame() # Initialize empty df\n",
    "             return self.processed_df\n",
    "\n",
    "        for idx, row in self.df.iterrows():\n",
    "            # Ensure required columns exist\n",
    "            if 'video_id' not in row or 'comments' not in row:\n",
    "                print(f\"Skipping row {idx}: Missing 'video_id' or 'comments' column.\")\n",
    "                continue\n",
    "\n",
    "            video_id = row['video_id']\n",
    "            comments_list = self.parse_comments_list(row['comments'])\n",
    "\n",
    "            for i, comment in enumerate(comments_list):\n",
    "                # Ensure comment is a non-empty string before processing\n",
    "                if isinstance(comment, str) and len(comment.strip()) > 0:\n",
    "                    comment_stripped = comment.strip() # Use stripped comment\n",
    "\n",
    "                    # Extract features\n",
    "                    features = self.extract_comment_features(comment_stripped)\n",
    "                    comment_type = self.classify_comment_type(comment_stripped)\n",
    "\n",
    "                    comment_data = {\n",
    "                        'video_id': video_id,\n",
    "                        'comment_position': i,\n",
    "                        'comment_text': comment_stripped,\n",
    "                        'comment_type': comment_type,\n",
    "                        **features\n",
    "                    }\n",
    "\n",
    "                    all_comments.append(comment_data)\n",
    "                    # video_comment_mapping.append((video_id, i)) # Not used\n",
    "\n",
    "        self.processed_df = pd.DataFrame(all_comments)\n",
    "        return self.processed_df\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    \"\"\"Custom dataset for comment analysis\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure text is a string, even if pandas returns something else sometimes\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            # Ensure labels are handled correctly, assuming they are integers\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class MultiTaskCommentAnalyzer(nn.Module):\n",
    "    \"\"\"Advanced multi-task model for comment analysis\"\"\"\n",
    "\n",
    "    # Increased default num_classes as classify_comment_type now has 7 categories\n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes=7, num_features=15):\n",
    "        super().__init__()\n",
    "\n",
    "        # BERT-based encoder\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Feature fusion layer - Check if features will always be provided\n",
    "        # Added a check or flexibility if features are not used\n",
    "        self.num_features = num_features # Store num_features\n",
    "        if num_features > 0:\n",
    "            self.feature_projection = nn.Linear(num_features, 128)\n",
    "            combined_size = self.bert.config.hidden_size + 128\n",
    "        else:\n",
    "            combined_size = self.bert.config.hidden_size\n",
    "            self.feature_projection = None # No projection needed\n",
    "\n",
    "\n",
    "        # Task-specific heads\n",
    "        # Use slightly different layer sizes for each task\n",
    "        self.comment_type_classifier = nn.Sequential(\n",
    "            nn.Linear(combined_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), # Increased dropout\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(combined_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), # Increased dropout\n",
    "            nn.Linear(256, 3)  # positive, negative, neutral (Needs labels for this task)\n",
    "        )\n",
    "\n",
    "        # Engagement predictor (Needs labels for this task, e.g., likes/replies count)\n",
    "        self.engagement_predictor = nn.Sequential(\n",
    "            nn.Linear(combined_size, 128), # Smaller layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), # Increased dropout\n",
    "            nn.Linear(128, 1) # Predicting a single continuous value (like count, reply count)\n",
    "        )\n",
    "\n",
    "        # Next comment topic predictor - This is more complex and usually requires sequence data\n",
    "        # The current implementation isn't suitable for the standard forward pass on independent comments.\n",
    "        # It would need a dedicated sequence model (like LSTM on comment embeddings over time).\n",
    "        # For this multi-task model operating on individual comments, the topic predictor isn't quite right here.\n",
    "        # Let's remove it from the standard forward pass and explain this.\n",
    "        # self.topic_predictor_lstm = nn.LSTM(combined_size, 256, batch_first=True, bidirectional=True)\n",
    "        # self.topic_output_linear = nn.Linear(512, num_classes) # Predicting next topic class\n",
    "\n",
    "        # Note: The 'next_topic' prediction part in this model's forward pass is commented out\n",
    "        # as it requires sequential input (a series of comments for a video),\n",
    "        # whereas the main forward pass is designed for processing individual comments.\n",
    "        # A separate sequence model or a modified forward pass handling batches of sequences would be needed.\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, features=None): # Removed sequence_mode flag\n",
    "        # BERT encoding\n",
    "        # Ensure bert_output is a tuple/object with pooler_output attribute\n",
    "        # Different BERT models might return different output types\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Use the pooled output (usually the [CLS] token representation)\n",
    "        if hasattr(bert_output, 'pooler_output') and bert_output.pooler_output is not None:\n",
    "             pooled_output = bert_output.pooler_output\n",
    "        # Fallback to the first token representation if pooler_output is None (e.g., some models)\n",
    "        elif hasattr(bert_output, 'last_hidden_state'):\n",
    "             pooled_output = bert_output.last_hidden_state[:, 0] # Take the [CLS] token output\n",
    "        else:\n",
    "             raise AttributeError(\"BERT model output does not have 'pooler_output' or 'last_hidden_state'.\")\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Feature fusion if provided and feature_projection exists\n",
    "        if features is not None and self.feature_projection is not None:\n",
    "            # Ensure features is the correct dtype (float)\n",
    "            features = features.float()\n",
    "            feature_embedding = self.feature_projection(features)\n",
    "            combined_features = torch.cat([pooled_output, feature_embedding], dim=1)\n",
    "        else:\n",
    "            combined_features = pooled_output\n",
    "\n",
    "        # Task outputs\n",
    "        comment_type_logits = self.comment_type_classifier(combined_features)\n",
    "        # Sentiment and Engagement outputs would require corresponding labels in the dataset\n",
    "        # For this example, we'll focus on the comment type classification\n",
    "        # sentiment_logits = self.sentiment_classifier(combined_features)\n",
    "        # engagement_score = self.engagement_predictor(combined_features)\n",
    "\n",
    "        # Return just the comment type for the main task\n",
    "        return {'comment_type': comment_type_logits}\n",
    "        # If training all tasks, return all:\n",
    "        # return {\n",
    "        #     'comment_type': comment_type_logits,\n",
    "        #     'sentiment': sentiment_logits,\n",
    "        #     'engagement': engagement_score\n",
    "        # }\n",
    "\n",
    "\n",
    "class NextCommentPredictor:\n",
    "    \"\"\"Specialized class for predicting next comment characteristics\"\"\"\n",
    "\n",
    "    def __init__(self, processed_df):\n",
    "        self.df = processed_df\n",
    "        self.video_sequences = {}\n",
    "        self.build_sequences()\n",
    "        # Initialize LabelEncoder for topics if needed\n",
    "        self.topic_encoder = LabelEncoder()\n",
    "        if not self.df.empty:\n",
    "            # Fit encoder on all unique comment types\n",
    "            self.topic_encoder.fit(self.df['comment_type'].unique())\n",
    "            self.num_topics = len(self.topic_encoder.classes_)\n",
    "        else:\n",
    "            self.num_topics = 0\n",
    "\n",
    "    def build_sequences(self):\n",
    "        \"\"\"Build comment sequences for each video\"\"\"\n",
    "        if self.df.empty:\n",
    "            print(\"DataFrame is empty. Cannot build sequences.\")\n",
    "            return\n",
    "        for video_id in self.df['video_id'].unique():\n",
    "            video_comments = self.df[self.df['video_id'] == video_id].sort_values('comment_position')\n",
    "            # Ensure the sequence has comment types\n",
    "            if 'comment_type' in video_comments.columns and not video_comments.empty:\n",
    "                 self.video_sequences[video_id] = video_comments\n",
    "            else:\n",
    "                 print(f\"Video {video_id} has no comments or no 'comment_type' column.\")\n",
    "\n",
    "\n",
    "    def create_sequence_dataset(self, sequence_length=5):\n",
    "        \"\"\"Create sequences for training next comment prediction\"\"\"\n",
    "        sequences = [] # Store encoded sequences of comment types\n",
    "        targets = []   # Store encoded target comment types\n",
    "\n",
    "        if not self.video_sequences:\n",
    "             print(\"No video sequences available.\")\n",
    "             return np.array([]), np.array([]) # Return empty arrays\n",
    "\n",
    "        for video_id, comments in self.video_sequences.items():\n",
    "            # Ensure comments DataFrame is not empty and has 'comment_type'\n",
    "            if comments.empty or 'comment_type' not in comments.columns:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Encode comment types to numerical labels\n",
    "                encoded_comment_types = self.topic_encoder.transform(comments['comment_type'].values)\n",
    "            except ValueError as e:\n",
    "                 print(f\"Error encoding comment types for video {video_id}: {e}. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            if len(encoded_comment_types) < sequence_length + 1:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(encoded_comment_types) - sequence_length):\n",
    "                seq = encoded_comment_types[i:i+sequence_length]\n",
    "                target = encoded_comment_types[i+sequence_length]\n",
    "                sequences.append(seq)\n",
    "                targets.append(target)\n",
    "\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    # This method is for *analyzing* existing sequences, not predicting with the model\n",
    "    def predict_next_based_on_frequency(self, sequence_prefix):\n",
    "        \"\"\"\n",
    "        Simple frequency-based prediction of the next topic\n",
    "        given a sequence prefix of comment types.\n",
    "        (This is a baseline, not using the neural network)\n",
    "        \"\"\"\n",
    "        if self.df.empty or self.num_topics == 0:\n",
    "             print(\"DataFrame empty or topics not encoded.\")\n",
    "             return None\n",
    "\n",
    "        # Encode the sequence prefix\n",
    "        try:\n",
    "            encoded_prefix = self.topic_encoder.transform(sequence_prefix)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error encoding sequence prefix: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        # Find all sequences in the dataset that start with this prefix\n",
    "        matching_targets = []\n",
    "        sequences, targets = self.create_sequence_dataset(sequence_length=len(encoded_prefix))\n",
    "\n",
    "        if sequences.size == 0: # Check if create_sequence_dataset returned empty arrays\n",
    "            print(\"No sequences generated from dataset.\")\n",
    "            return None\n",
    "\n",
    "        # Convert sequences to tuples for easy comparison\n",
    "        sequence_tuples = [tuple(s) for s in sequences]\n",
    "        encoded_prefix_tuple = tuple(encoded_prefix)\n",
    "\n",
    "        # Collect targets that immediately follow the prefix\n",
    "        for i in range(len(sequence_tuples)):\n",
    "            # Check if the sequence starts with the prefix\n",
    "            if sequence_tuples[i][:len(encoded_prefix_tuple)] == encoded_prefix_tuple:\n",
    "                 # The target for this sequence *is* the element immediately following the prefix\n",
    "                 # in the original dataset's sequences where the prefix appeared.\n",
    "                 # We need to find the element *after* the sequence of length len(encoded_prefix_tuple)\n",
    "                 # However, the create_sequence_dataset method already aligns targets correctly.\n",
    "                 # The target 'targets[i]' is the comment type *after* the sequence 'sequences[i]'.\n",
    "                 # So we just need to match sequences[i] exactly to our encoded_prefix.\n",
    "                 # A more accurate method would be to look at sequences of length N+1 and find where the first N elements match.\n",
    "                 # Let's adjust create_sequence_dataset or this logic.\n",
    "\n",
    "                 # Simpler approach: Iterate through original video sequences\n",
    "                 # This is less efficient but conceptually matches finding the topic after a prefix.\n",
    "                 pass # We'll implement this below\n",
    "\n",
    "        # More efficient prefix matching directly on original sequences data\n",
    "        next_topics_after_prefix = []\n",
    "        prefix_len = len(encoded_prefix)\n",
    "\n",
    "        for video_id, comments in self.video_sequences.items():\n",
    "            if 'comment_type' not in comments.columns: continue\n",
    "\n",
    "            try:\n",
    "                encoded_video_sequence = self.topic_encoder.transform(comments['comment_type'].values)\n",
    "            except ValueError:\n",
    "                continue # Skip if encoding fails\n",
    "\n",
    "            for i in range(len(encoded_video_sequence) - prefix_len):\n",
    "                if np.array_equal(encoded_video_sequence[i : i + prefix_len], encoded_prefix):\n",
    "                    if i + prefix_len < len(encoded_video_sequence):\n",
    "                        next_topics_after_prefix.append(encoded_video_sequence[i + prefix_len])\n",
    "\n",
    "        if not next_topics_after_prefix:\n",
    "            print(f\"No occurrences found for sequence prefix: {sequence_prefix}\")\n",
    "            # Fallback: return the overall most frequent topic\n",
    "            if self.num_topics > 0:\n",
    "                 overall_topics = self.df['comment_type'].unique()\n",
    "                 overall_counts = self.df['comment_type'].value_counts()\n",
    "                 most_common_overall = overall_counts.index[0]\n",
    "                 print(f\"Returning overall most frequent topic: {most_common_overall}\")\n",
    "                 return most_common_overall\n",
    "            else:\n",
    "                 return None # Cannot fallback\n",
    "\n",
    "\n",
    "        # Count occurrences of topics that follow the prefix\n",
    "        next_topic_counts = Counter(next_topics_after_prefix)\n",
    "\n",
    "        if not next_topic_counts:\n",
    "             print(\"No topics found following the sequence prefix.\")\n",
    "             return None\n",
    "\n",
    "        # Get the most common topic label (encoded)\n",
    "        most_likely_next_topic_encoded = next_topic_counts.most_common(1)[0][0]\n",
    "\n",
    "        # Decode the label\n",
    "        most_likely_next_topic = self.topic_encoder.inverse_transform([most_likely_next_topic_encoded])[0]\n",
    "\n",
    "        return most_likely_next_topic\n",
    "\n",
    "\n",
    "class CommentAnalysisPipeline:\n",
    "    \"\"\"Complete pipeline for comment analysis and prediction\"\"\"\n",
    "\n",
    "    def __init__(self, data_path=None, df=None):\n",
    "        self.processor = YouTubeCommentsProcessor(csv_path=data_path, df=df)\n",
    "        # Consider adding 'do_lower_case=True' if needed based on model pre-training\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.processed_df = None # Initialize processed_df\n",
    "        self.feature_columns = [] # To store feature column names\n",
    "        self.predictor = None # Initialize predictor\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare and process the dataset\"\"\"\n",
    "        print(\"Processing comments dataset...\")\n",
    "        self.processed_df = self.processor.process_dataset()\n",
    "\n",
    "        if self.processed_df.empty:\n",
    "            print(\"Processing resulted in an empty DataFrame. Cannot proceed.\")\n",
    "            return self.processed_df\n",
    "\n",
    "        print(f\"Total comments processed: {len(self.processed_df)}\")\n",
    "        print(f\"Comment types distribution:\")\n",
    "        print(self.processed_df['comment_type'].value_counts())\n",
    "\n",
    "        # Identify feature columns after processing\n",
    "        self.feature_columns = [col for col in self.processed_df.columns\n",
    "                               if col not in ['video_id', 'comment_position', 'comment_text', 'comment_type']]\n",
    "\n",
    "        # Initialize the sequence predictor now that processed_df is ready\n",
    "        self.predictor = NextCommentPredictor(self.processed_df)\n",
    "\n",
    "        return self.processed_df\n",
    "\n",
    "    def train_model(self, test_size=0.2, batch_size=16, epochs=3):\n",
    "        \"\"\"Train the multi-task model (currently only comment type)\"\"\"\n",
    "        if self.processed_df is None or self.processed_df.empty:\n",
    "            print(\"Data not processed. Run prepare_data() first.\")\n",
    "            return None\n",
    "\n",
    "        # Prepare features\n",
    "        # Ensure feature columns exist in the processed df\n",
    "        available_feature_cols = [col for col in self.feature_columns if col in self.processed_df.columns]\n",
    "\n",
    "        if not available_feature_cols:\n",
    "             print(\"Warning: No feature columns found for training.\")\n",
    "             X_features = np.zeros((len(self.processed_df), 0)) # Empty feature array\n",
    "             num_features = 0\n",
    "        else:\n",
    "             X_features = self.processed_df[available_feature_cols].values\n",
    "             # Handle potential non-numeric data in features (shouldn't happen with current features, but good practice)\n",
    "             # X_features = X_features.astype(float) # Ensure float type\n",
    "             num_features = X_features.shape[1]\n",
    "\n",
    "\n",
    "        X_text = self.processed_df['comment_text'].values\n",
    "        y = self.label_encoder.fit_transform(self.processed_df['comment_type'].values)\n",
    "\n",
    "        # Check if there are enough samples for splitting\n",
    "        if len(X_text) < 2:\n",
    "             print(\"Not enough data samples to perform train-test split.\")\n",
    "             return None\n",
    "        if len(np.unique(y)) < 2:\n",
    "             print(\"Only one class found. Cannot perform classification training.\")\n",
    "             return None\n",
    "        if len(np.unique(y)) < 2 / test_size: # Heuristic check for meaningful stratification\n",
    "             print(f\"Warning: Very few samples per class for test_size={test_size}. Stratification might be inaccurate.\")\n",
    "\n",
    "\n",
    "        # Train-test split\n",
    "        # Stratify requires at least one sample per class in both train and test sets.\n",
    "        # If a class has only 1 sample, stratification will fail.\n",
    "        try:\n",
    "            X_text_train, X_text_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(\n",
    "                X_text, X_features, y, test_size=test_size, random_state=42, stratify=y\n",
    "            )\n",
    "        except ValueError as e:\n",
    "             print(f\"Error during train-test split (stratify failed): {e}\")\n",
    "             print(\"Try reducing test_size or handle classes with very few samples.\")\n",
    "             # Fallback to non-stratified split or skip training\n",
    "             print(\"Attempting non-stratified split...\")\n",
    "             try:\n",
    "                  X_text_train, X_text_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(\n",
    "                      X_text, X_features, y, test_size=test_size, random_state=42\n",
    "                  )\n",
    "                  print(\"Non-stratified split successful.\")\n",
    "             except Exception as non_strat_e:\n",
    "                  print(f\"Non-stratified split also failed: {non_strat_e}\")\n",
    "                  return None # Cannot train if split fails\n",
    "\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = CommentDataset(X_text_train, y_train, self.tokenizer)\n",
    "        test_dataset = CommentDataset(X_text_test, y_test, self.tokenizer)\n",
    "\n",
    "        # Use the corresponding features for the dataset\n",
    "        # Note: The current CommentDataset only takes texts and labels, not features.\n",
    "        # To train with features, the Dataset class needs to be modified or features passed separately to the DataLoader.\n",
    "        # For now, the model's forward expects features=None unless passed.\n",
    "        # Let's modify the Dataset and DataLoader to handle features.\n",
    "\n",
    "        class CommentDatasetWithFeatures(Dataset):\n",
    "            def __init__(self, texts, features, labels, tokenizer, max_length=128):\n",
    "                self.texts = texts\n",
    "                self.features = features\n",
    "                self.labels = labels\n",
    "                self.tokenizer = tokenizer\n",
    "                self.max_length = max_length\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.texts)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                text = str(self.texts[idx])\n",
    "                feature_row = self.features[idx]\n",
    "\n",
    "                encoding = self.tokenizer(\n",
    "                    text,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "\n",
    "                return {\n",
    "                    'input_ids': encoding['input_ids'].flatten(),\n",
    "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                    'features': torch.tensor(feature_row, dtype=torch.float), # Ensure float type\n",
    "                    'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "                }\n",
    "\n",
    "        train_dataset = CommentDatasetWithFeatures(X_text_train, X_feat_train, y_train, self.tokenizer)\n",
    "        test_dataset = CommentDatasetWithFeatures(X_text_test, X_feat_test, y_test, self.tokenizer)\n",
    "\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Initialize model\n",
    "        num_classes = len(self.label_encoder.classes_)\n",
    "        self.model = MultiTaskCommentAnalyzer(num_classes=num_classes, num_features=num_features)\n",
    "\n",
    "        # Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Training setup\n",
    "        optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
    "        # Only using CrossEntropyLoss for the comment type task for now\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                features = batch['features'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    features=features # Pass features to the model\n",
    "                )\n",
    "\n",
    "                # Calculate loss for the main task (comment_type)\n",
    "                loss = criterion(outputs['comment_type'], labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Evaluation\n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        self.evaluate_model(test_loader, device) # Pass device to evaluation\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate_model(self, test_loader, device):\n",
    "        \"\"\"Evaluate the trained model\"\"\"\n",
    "        if self.model is None:\n",
    "             print(\"Model not trained yet.\")\n",
    "             return [], []\n",
    "        if self.processed_df.empty:\n",
    "             print(\"Data not processed. Cannot evaluate.\")\n",
    "             return [], []\n",
    "\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        all_preds = [] # Store raw predictions for confidence analysis etc.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                features = batch['features'].to(device) # Pass features\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    features=features # Pass features\n",
    "                )\n",
    "\n",
    "                # Assuming comment_type is the primary task evaluation\n",
    "                logits = outputs['comment_type']\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(logits.cpu().numpy()) # Store raw logits\n",
    "\n",
    "\n",
    "        # Classification report\n",
    "        if hasattr(self, 'label_encoder') and self.label_encoder is not None:\n",
    "             class_names = self.label_encoder.classes_\n",
    "             print(\"\\nClassification Report (Comment Type):\")\n",
    "             try:\n",
    "                print(classification_report(true_labels, predictions, target_names=class_names, zero_division=0))\n",
    "             except ValueError as e:\n",
    "                 print(f\"Could not generate classification report: {e}\")\n",
    "                 print(\"This can happen if true_labels or predictions contain classes not in target_names.\")\n",
    "                 # Fallback or print raw arrays for debugging\n",
    "                 # print(\"True labels:\", true_labels)\n",
    "                 # print(\"Predictions:\", predictions)\n",
    "                 # print(\"Class names:\", class_names)\n",
    "\n",
    "        else:\n",
    "             print(\"LabelEncoder not initialized. Cannot print class names.\")\n",
    "             print(\"\\nClassification Report (Comment Type):\")\n",
    "             print(classification_report(true_labels, predictions, zero_division=0))\n",
    "\n",
    "\n",
    "        # Confusion Matrix (Optional but helpful visualization)\n",
    "        # try:\n",
    "        #     cm = confusion_matrix(true_labels, predictions)\n",
    "        #     plt.figure(figsize=(10, 8))\n",
    "        #     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        #     plt.xlabel('Predicted Label')\n",
    "        #     plt.ylabel('True Label')\n",
    "        #     plt.title('Confusion Matrix (Comment Type)')\n",
    "        #     plt.show()\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Could not generate confusion matrix plot: {e}\")\n",
    "\n",
    "\n",
    "        return predictions, true_labels\n",
    "\n",
    "    def predict_next_comment_topic(self, video_id, current_sequence_comments, sequence_length=5):\n",
    "        \"\"\"\n",
    "        Predict the topic of the next comment given a sequence of recent comments\n",
    "        for a specific video using the frequency-based predictor.\n",
    "\n",
    "        Args:\n",
    "            video_id (str): The ID of the video.\n",
    "            current_sequence_comments (list): A list of string comments forming the sequence.\n",
    "            sequence_length (int): The length of the sequence to consider (for simple frequency).\n",
    "                                   Note: The frequency predictor uses the *exact* provided sequence length.\n",
    "\n",
    "        Returns:\n",
    "            str: The predicted topic of the next comment, or None if prediction fails.\n",
    "        \"\"\"\n",
    "        if self.predictor is None:\n",
    "            print(\"Predictor not initialized. Run prepare_data() first.\")\n",
    "            return None\n",
    "\n",
    "        if not current_sequence_comments:\n",
    "             print(\"Current sequence is empty. Cannot predict.\")\n",
    "             return None\n",
    "\n",
    "        # Use only the last 'sequence_length' comments if the provided list is longer\n",
    "        sequence_to_use = current_sequence_comments[-sequence_length:]\n",
    "\n",
    "        print(f\"Predicting next topic based on sequence: {sequence_to_use}\")\n",
    "\n",
    "        # The NextCommentPredictor requires the sequence of *types*, not text.\n",
    "        # We need to classify the types of the input comments first.\n",
    "        predicted_types_sequence = [self.processor.classify_comment_type(comment) for comment in sequence_to_use]\n",
    "\n",
    "        # Now use the frequency predictor with the sequence of predicted types\n",
    "        predicted_next_topic = self.predictor.predict_next_based_on_frequency(predicted_types_sequence)\n",
    "\n",
    "        if predicted_next_topic:\n",
    "             print(f\"Predicted next topic: {predicted_next_topic}\")\n",
    "        else:\n",
    "             print(\"Prediction failed or no matching sequence found.\")\n",
    "\n",
    "        return predicted_next_topic\n",
    "\n",
    "\n",
    "    def analyze_comment_patterns(self):\n",
    "        \"\"\"Analyze patterns in the comment data\"\"\"\n",
    "        if self.processed_df is None or self.processed_df.empty:\n",
    "            print(\"Data not processed. Run prepare_data() first.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n=== Comment Analysis Report ===\")\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "        # Basic statistics\n",
    "        print(f\"Total videos: {self.processed_df['video_id'].nunique()}\")\n",
    "        print(f\"Total comments: {len(self.processed_df)}\")\n",
    "        if self.processed_df['video_id'].nunique() > 0:\n",
    "             print(f\"Average comments per video: {len(self.processed_df) / self.processed_df['video_id'].nunique():.2f}\")\n",
    "        else:\n",
    "             print(\"Average comments per video: N/A (No videos found)\")\n",
    "\n",
    "        # Comment type distribution\n",
    "        print(\"\\nComment Type Distribution:\")\n",
    "        type_counts = self.processed_df['comment_type'].value_counts()\n",
    "        if not type_counts.empty:\n",
    "            total_comments = len(self.processed_df)\n",
    "            for comment_type, count in type_counts.items():\n",
    "                print(f\"  {comment_type}: {count} ({count/total_comments*100:.1f}%)\")\n",
    "            # Optional: Plot distribution\n",
    "            # plt.figure(figsize=(10, 6))\n",
    "            # sns.countplot(data=self.processed_df, y='comment_type', order=type_counts.index, palette='viridis')\n",
    "            # plt.title('Distribution of Comment Types')\n",
    "            # plt.xlabel('Count')\n",
    "            # plt.ylabel('Comment Type')\n",
    "            # plt.show()\n",
    "        else:\n",
    "             print(\"  No comment types found.\")\n",
    "\n",
    "\n",
    "        # Length analysis\n",
    "        print(f\"\\nComment Length Statistics:\")\n",
    "        if not self.processed_df.empty and 'length' in self.processed_df.columns:\n",
    "             print(f\"  Average length: {self.processed_df['length'].mean():.2f} characters\")\n",
    "             print(f\"  Median length: {self.processed_df['length'].median():.2f} characters\")\n",
    "             # Optional: Plot distribution\n",
    "             # plt.figure(figsize=(8, 4))\n",
    "             # sns.histplot(self.processed_df['length'], bins=50, kde=True)\n",
    "             # plt.title('Distribution of Comment Lengths')\n",
    "             # plt.xlabel('Length (characters)')\n",
    "             # plt.ylabel('Frequency')\n",
    "             # plt.show()\n",
    "        else:\n",
    "             print(\"  Length statistics not available.\")\n",
    "\n",
    "\n",
    "        # Topic analysis (based on keyword scores)\n",
    "        print(f\"\\nTopic Keyword Analysis:\")\n",
    "        if not self.processed_df.empty:\n",
    "            prod_count = (self.processed_df['productivity_score'] > 0).sum() if 'productivity_score' in self.processed_df.columns else 0\n",
    "            fin_count = (self.processed_df['finance_score'] > 0).sum() if 'finance_score' in self.processed_df.columns else 0\n",
    "            mot_count = (self.processed_df['motivation_score'] > 0).sum() if 'motivation_score' in self.processed_df.columns else 0\n",
    "\n",
    "            print(f\"  Productivity-related (score > 0): {prod_count} comments\")\n",
    "            print(f\"  Finance-related (score > 0): {fin_count} comments\")\n",
    "            print(f\"  Motivation-related (score > 0): {mot_count} comments\")\n",
    "        else:\n",
    "             print(\"  Topic keyword analysis not available.\")\n",
    "\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "\n",
    "# Example usage and execution\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"üöÄ Starting YouTube Comments AI Analysis Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # --- DEMO SETUP ---\n",
    "    # For demonstration, creating sample structure similar to expected input\n",
    "    # In a real scenario, you would load your data from a CSV.\n",
    "    # Example using a simple CSV file structure:\n",
    "    # video_id,comments\n",
    "    # video_a,['Great video!', 'Thanks for this.', 'How can I do X?']\n",
    "    # video_b,[\"Amazing content!\", \"I disagree.\", \"This is really bad.\"]\n",
    "    # video_c,\"['Personal story here...', 'Another comment', 'spam_indicator like betterhelp']\"\n",
    "\n",
    "\n",
    "    # Create a dummy CSV file for demonstration\n",
    "    dummy_csv_path = \"dummy_comments.csv\"\n",
    "    dummy_data_content = \"\"\"video_id,comments\n",
    "video_1,\"['Great video, thanks for sharing!', 'This helped me a lot with my productivity.', 'Question: How did you edit this part?', 'I hate this part.', 'check out my channel for similar content']\"\n",
    "video_2,\"['Amazing content!', 'What camera do you use?', 'My personal experience was different.', 'Thank you so much!', 'This is the best!', 'another great comment']\"\n",
    "video_3,\"['Short comment.', 'Another short comment!', 'Detailed feedback: This video was very informative, but I think you could improve the audio quality. The topic on finance was particularly interesting, but it could be more in-depth. Overall good though.']\"\n",
    "video_4,\"['Spam link www.mysite.com', 'Lois E Wilson is a scammer', 'Black algorithm problems']\"\n",
    "\"\"\"\n",
    "    try:\n",
    "        with open(dummy_csv_path, \"w\") as f:\n",
    "            f.write(dummy_data_content)\n",
    "        print(f\"Created dummy data file: {dummy_csv_path}\")\n",
    "\n",
    "        # Initialize pipeline using the dummy CSV path\n",
    "        pipeline = CommentAnalysisPipeline(data_path=dummy_csv_path)\n",
    "\n",
    "        # Process data\n",
    "        processed_df = pipeline.prepare_data()\n",
    "\n",
    "        if not processed_df.empty:\n",
    "            # Analyze patterns\n",
    "            pipeline.analyze_comment_patterns()\n",
    "\n",
    "            # --- MODEL TRAINING (Optional for analysis, requires sufficient data) ---\n",
    "            # Note: Training a BERT-based model requires significant resources (RAM, potentially GPU)\n",
    "            # and a substantial amount of labeled data for meaningful results.\n",
    "            # The dummy data is too small for effective training.\n",
    "            # This part is commented out for demonstration with small dummy data.\n",
    "\n",
    "            # print(\"\\nüî¨ Training Model (using dummy data - results may not be meaningful)...\")\n",
    "            # # You might need more epochs and a larger batch size for real data\n",
    "            # # Ensure you have enough data for a split (at least 2 samples)\n",
    "            # if len(processed_df) > 10: # Simple check if there's *some* data\n",
    "            #     model = pipeline.train_model(epochs=1, batch_size=8) # Use smaller batch/epochs for demo\n",
    "            #     if model:\n",
    "            #         print(\"\\nModel training complete.\")\n",
    "            #     else:\n",
    "            #         print(\"\\nModel training skipped due to data issues.\")\n",
    "            # else:\n",
    "            #      print(\"\\nSkipping model training: Not enough data in processed_df.\")\n",
    "\n",
    "\n",
    "            # --- NEXT COMMENT PREDICTION DEMO (using frequency baseline) ---\n",
    "            print(\"\\nüîÆ Demonstrating Next Comment Prediction (Frequency Baseline)\")\n",
    "            # Example sequence from video_1: 'Great video, thanks for sharing!', 'This helped me a lot with my productivity.'\n",
    "            video_id_to_predict = 'video_1'\n",
    "            # Find the sequence of comments from the processed_df for video_1\n",
    "            video_1_comments_df = processed_df[processed_df['video_id'] == video_id_to_predict].sort_values('comment_position')\n",
    "\n",
    "            if not video_1_comments_df.empty:\n",
    "                 # Use the first 2 comments as the sequence prefix\n",
    "                 demo_sequence = video_1_comments_df['comment_text'].tolist()[:2]\n",
    "                 if len(demo_sequence) == 2:\n",
    "                     print(f\"\\nPredicting next topic for video '{video_id_to_predict}' based on sequence: {demo_sequence}\")\n",
    "                     predicted_topic = pipeline.predict_next_comment_topic(video_id_to_predict, demo_sequence)\n",
    "                     if predicted_topic:\n",
    "                         print(f\"Predicted next topic type (based on frequency): {predicted_topic}\")\n",
    "                     else:\n",
    "                         print(\"Prediction failed.\")\n",
    "                 else:\n",
    "                     print(f\"Not enough comments in video '{video_id_to_predict}' to form a sequence of length 2.\")\n",
    "            else:\n",
    "                 print(f\"No comments found for video '{video_id_to_predict}' in processed data.\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"\\nPipeline stopped because processed data is empty.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dummy data file not found at {dummy_csv_path}\")\n",
    "        print(\"Please ensure the script can create/access this file, or provide your own data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during the main execution: {e}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Pipeline setup complete (execution finished).\")\n",
    "    print(\"Remember to fix your PyTorch installation if you encountered the NameError.\")\n",
    "    print(\"To use with your own CSV data:\")\n",
    "    print(\"1. Ensure your CSV has 'video_id' and 'comments' columns.\")\n",
    "    print(\"2. Initialize: pipeline = CommentAnalysisPipeline(data_path='path/to/your/file.csv')\")\n",
    "    print(\"3. Process: processed_df = pipeline.prepare_data()\")\n",
    "    print(\"4. Train (Optional, needs data): model = pipeline.train_model()\")\n",
    "    print(\"5. Analyze: pipeline.analyze_comment_patterns()\")\n",
    "    print(\"6. Predict Next Topic (Optional): pipeline.predict_next_comment_topic('your_video_id', ['comment1', 'comment2'])\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94433529",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torch torchaudio torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30959f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>favouriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QF2wIywvDhk</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>I¬†Tried AI as¬†a¬†Life Coach for 365 Days - Here...</td>\n",
       "      <td>#ad Going to therapy is a sign of strength, no...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal']</td>\n",
       "      <td>2025-05-31T13:00:44Z</td>\n",
       "      <td>29534</td>\n",
       "      <td>1323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145</td>\n",
       "      <td>PT36M42S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-r2SH6EH5eY</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Getting Rich is a Game. Here‚Äôs How to Win.</td>\n",
       "      <td>Sign up and download Grammarly for FREE. If yo...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal']</td>\n",
       "      <td>2025-05-28T13:30:01Z</td>\n",
       "      <td>101554</td>\n",
       "      <td>4581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>312</td>\n",
       "      <td>PT43M12S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmCaQxk4b8c</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Here's How to Get to the Top 1% (Discipline Is...</td>\n",
       "      <td>Check out Spotter to brainstorm better video i...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal', 'Joe Hudson']</td>\n",
       "      <td>2025-05-09T16:23:47Z</td>\n",
       "      <td>305074</td>\n",
       "      <td>11357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>609</td>\n",
       "      <td>PT1H9M34S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k_K9MbqNhA0</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>17 Cheap Purchases That Actually Improved My Life</td>\n",
       "      <td>Check out DeleteMe using my link and get 20% o...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal']</td>\n",
       "      <td>2025-05-03T13:01:19Z</td>\n",
       "      <td>191858</td>\n",
       "      <td>5755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360</td>\n",
       "      <td>PT16M25S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X_XNSeW9jok</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>How To Get Rich</td>\n",
       "      <td>üíå Join LifeNotes, my weekly email where I shar...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal']</td>\n",
       "      <td>2025-04-18T15:00:51Z</td>\n",
       "      <td>645630</td>\n",
       "      <td>29902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1795</td>\n",
       "      <td>PT28M7S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>FEUIxnnW5-w</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>When I Was Your Man (Bruno Mars) - Duranka Per...</td>\n",
       "      <td>Duranka came over to Cambridge last night to w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-28T18:14:19Z</td>\n",
       "      <td>23920</td>\n",
       "      <td>306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>PT3M33S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>d2sMEE1NrFc</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Thinking Out Loud (Ed Sheeran) - Ali Abdaal &amp; ...</td>\n",
       "      <td>Another (quite bassy) cover of one of our favo...</td>\n",
       "      <td>['thinking out loud', 'ed sheeran', 'acoustic ...</td>\n",
       "      <td>2017-01-18T07:13:58Z</td>\n",
       "      <td>8736</td>\n",
       "      <td>193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>PT3M42S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>7APfFjfnNBc</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Payphone (Maroon 5) - Katherine Macfarland &amp; A...</td>\n",
       "      <td>First cover of 2017! We both love this song, s...</td>\n",
       "      <td>['payphone', 'maroon 5', 'acoustic cover', 'co...</td>\n",
       "      <td>2017-01-04T23:13:51Z</td>\n",
       "      <td>103826</td>\n",
       "      <td>2429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162</td>\n",
       "      <td>PT2M35S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>XJHM0fEH3ss</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Rolling in the Deep (Adele) - Duranka Perera &amp;...</td>\n",
       "      <td>Our first 'proper' video! One of our all-time ...</td>\n",
       "      <td>['rolling in the deep', 'adele', 'cover', 'aco...</td>\n",
       "      <td>2016-04-05T09:05:29Z</td>\n",
       "      <td>72610</td>\n",
       "      <td>857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>PT2M29S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>LQ3Mu8A7gjY</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>All of Me (John Legend) - Duranka Perera</td>\n",
       "      <td>First attempt at playing around with this sort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-27T19:30:11Z</td>\n",
       "      <td>158771</td>\n",
       "      <td>1844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>PT1M43S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1078 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id channelTitle  \\\n",
       "0     QF2wIywvDhk   Ali Abdaal   \n",
       "1     -r2SH6EH5eY   Ali Abdaal   \n",
       "2     lmCaQxk4b8c   Ali Abdaal   \n",
       "3     k_K9MbqNhA0   Ali Abdaal   \n",
       "4     X_XNSeW9jok   Ali Abdaal   \n",
       "...           ...          ...   \n",
       "1073  FEUIxnnW5-w   Ali Abdaal   \n",
       "1074  d2sMEE1NrFc   Ali Abdaal   \n",
       "1075  7APfFjfnNBc   Ali Abdaal   \n",
       "1076  XJHM0fEH3ss   Ali Abdaal   \n",
       "1077  LQ3Mu8A7gjY   Ali Abdaal   \n",
       "\n",
       "                                                  title  \\\n",
       "0     I¬†Tried AI as¬†a¬†Life Coach for 365 Days - Here...   \n",
       "1            Getting Rich is a Game. Here‚Äôs How to Win.   \n",
       "2     Here's How to Get to the Top 1% (Discipline Is...   \n",
       "3     17 Cheap Purchases That Actually Improved My Life   \n",
       "4                                       How To Get Rich   \n",
       "...                                                 ...   \n",
       "1073  When I Was Your Man (Bruno Mars) - Duranka Per...   \n",
       "1074  Thinking Out Loud (Ed Sheeran) - Ali Abdaal & ...   \n",
       "1075  Payphone (Maroon 5) - Katherine Macfarland & A...   \n",
       "1076  Rolling in the Deep (Adele) - Duranka Perera &...   \n",
       "1077           All of Me (John Legend) - Duranka Perera   \n",
       "\n",
       "                                            description  \\\n",
       "0     #ad Going to therapy is a sign of strength, no...   \n",
       "1     Sign up and download Grammarly for FREE. If yo...   \n",
       "2     Check out Spotter to brainstorm better video i...   \n",
       "3     Check out DeleteMe using my link and get 20% o...   \n",
       "4     üíå Join LifeNotes, my weekly email where I shar...   \n",
       "...                                                 ...   \n",
       "1073  Duranka came over to Cambridge last night to w...   \n",
       "1074  Another (quite bassy) cover of one of our favo...   \n",
       "1075  First cover of 2017! We both love this song, s...   \n",
       "1076  Our first 'proper' video! One of our all-time ...   \n",
       "1077  First attempt at playing around with this sort...   \n",
       "\n",
       "                                                   tags           publishedAt  \\\n",
       "0                           ['Ali Abdaal', 'Ali abdal']  2025-05-31T13:00:44Z   \n",
       "1                           ['Ali Abdaal', 'Ali abdal']  2025-05-28T13:30:01Z   \n",
       "2             ['Ali Abdaal', 'Ali abdal', 'Joe Hudson']  2025-05-09T16:23:47Z   \n",
       "3                           ['Ali Abdaal', 'Ali abdal']  2025-05-03T13:01:19Z   \n",
       "4                           ['Ali Abdaal', 'Ali abdal']  2025-04-18T15:00:51Z   \n",
       "...                                                 ...                   ...   \n",
       "1073                                                NaN  2017-01-28T18:14:19Z   \n",
       "1074  ['thinking out loud', 'ed sheeran', 'acoustic ...  2017-01-18T07:13:58Z   \n",
       "1075  ['payphone', 'maroon 5', 'acoustic cover', 'co...  2017-01-04T23:13:51Z   \n",
       "1076  ['rolling in the deep', 'adele', 'cover', 'aco...  2016-04-05T09:05:29Z   \n",
       "1077                                                NaN  2016-03-27T19:30:11Z   \n",
       "\n",
       "      viewCount  likeCount  favouriteCount  commentCount   duration  \\\n",
       "0         29534       1323             NaN           145   PT36M42S   \n",
       "1        101554       4581             NaN           312   PT43M12S   \n",
       "2        305074      11357             NaN           609  PT1H9M34S   \n",
       "3        191858       5755             NaN           360   PT16M25S   \n",
       "4        645630      29902             NaN          1795    PT28M7S   \n",
       "...         ...        ...             ...           ...        ...   \n",
       "1073      23920        306             NaN            24    PT3M33S   \n",
       "1074       8736        193             NaN            46    PT3M42S   \n",
       "1075     103826       2429             NaN           162    PT2M35S   \n",
       "1076      72610        857             NaN            48    PT2M29S   \n",
       "1077     158771       1844             NaN           187    PT1M43S   \n",
       "\n",
       "     definition  caption  \n",
       "0            hd    False  \n",
       "1            hd    False  \n",
       "2            hd    False  \n",
       "3            hd    False  \n",
       "4            hd    False  \n",
       "...         ...      ...  \n",
       "1073         hd    False  \n",
       "1074         hd    False  \n",
       "1075         hd    False  \n",
       "1076         hd    False  \n",
       "1077         hd    False  \n",
       "\n",
       "[1078 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=pd.read_csv(r\"C:\\Users\\hafss\\OneDrive\\Desktop\\PFA_2A\\youtube\\data\\video_details_raw.csv\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38de725c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>favouriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>publishDayName</th>\n",
       "      <th>durationSecs</th>\n",
       "      <th>tagCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QF2wIywvDhk</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>I¬†Tried AI as¬†a¬†Life Coach for 365 Days - Here...</td>\n",
       "      <td>#ad Going to therapy is a sign of strength, no...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal']</td>\n",
       "      <td>2025-05-31 13:00:44+00:00</td>\n",
       "      <td>29534</td>\n",
       "      <td>1323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145</td>\n",
       "      <td>PT36M42S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-r2SH6EH5eY</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Getting Rich is a Game. Here‚Äôs How to Win.</td>\n",
       "      <td>Sign up and download Grammarly for FREE. If yo...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal']</td>\n",
       "      <td>2025-05-28 13:30:01+00:00</td>\n",
       "      <td>101554</td>\n",
       "      <td>4581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>312</td>\n",
       "      <td>PT43M12S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmCaQxk4b8c</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Here's How to Get to the Top 1% (Discipline Is...</td>\n",
       "      <td>Check out Spotter to brainstorm better video i...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal', 'Joe Hudson']</td>\n",
       "      <td>2025-05-09 16:23:47+00:00</td>\n",
       "      <td>305074</td>\n",
       "      <td>11357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>609</td>\n",
       "      <td>PT1H9M34S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4174.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k_K9MbqNhA0</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>17 Cheap Purchases That Actually Improved My Life</td>\n",
       "      <td>Check out DeleteMe using my link and get 20% o...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal']</td>\n",
       "      <td>2025-05-03 13:01:19+00:00</td>\n",
       "      <td>191858</td>\n",
       "      <td>5755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360</td>\n",
       "      <td>PT16M25S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>985.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X_XNSeW9jok</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>How To Get Rich</td>\n",
       "      <td>üíå Join LifeNotes, my weekly email where I shar...</td>\n",
       "      <td>['Ali Abdaal', 'Ali abdal']</td>\n",
       "      <td>2025-04-18 15:00:51+00:00</td>\n",
       "      <td>645630</td>\n",
       "      <td>29902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1795</td>\n",
       "      <td>PT28M7S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>FEUIxnnW5-w</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>When I Was Your Man (Bruno Mars) - Duranka Per...</td>\n",
       "      <td>Duranka came over to Cambridge last night to w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-28 18:14:19+00:00</td>\n",
       "      <td>23920</td>\n",
       "      <td>306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>PT3M33S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>213.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>d2sMEE1NrFc</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Thinking Out Loud (Ed Sheeran) - Ali Abdaal &amp; ...</td>\n",
       "      <td>Another (quite bassy) cover of one of our favo...</td>\n",
       "      <td>['thinking out loud', 'ed sheeran', 'acoustic ...</td>\n",
       "      <td>2017-01-18 07:13:58+00:00</td>\n",
       "      <td>8736</td>\n",
       "      <td>193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>PT3M42S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>7APfFjfnNBc</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Payphone (Maroon 5) - Katherine Macfarland &amp; A...</td>\n",
       "      <td>First cover of 2017! We both love this song, s...</td>\n",
       "      <td>['payphone', 'maroon 5', 'acoustic cover', 'co...</td>\n",
       "      <td>2017-01-04 23:13:51+00:00</td>\n",
       "      <td>103826</td>\n",
       "      <td>2429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162</td>\n",
       "      <td>PT2M35S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>XJHM0fEH3ss</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>Rolling in the Deep (Adele) - Duranka Perera &amp;...</td>\n",
       "      <td>Our first 'proper' video! One of our all-time ...</td>\n",
       "      <td>['rolling in the deep', 'adele', 'cover', 'aco...</td>\n",
       "      <td>2016-04-05 09:05:29+00:00</td>\n",
       "      <td>72610</td>\n",
       "      <td>857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>PT2M29S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>LQ3Mu8A7gjY</td>\n",
       "      <td>Ali Abdaal</td>\n",
       "      <td>All of Me (John Legend) - Duranka Perera</td>\n",
       "      <td>First attempt at playing around with this sort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-27 19:30:11+00:00</td>\n",
       "      <td>158771</td>\n",
       "      <td>1844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>PT1M43S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1078 rows √ó 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id channelTitle  \\\n",
       "0     QF2wIywvDhk   Ali Abdaal   \n",
       "1     -r2SH6EH5eY   Ali Abdaal   \n",
       "2     lmCaQxk4b8c   Ali Abdaal   \n",
       "3     k_K9MbqNhA0   Ali Abdaal   \n",
       "4     X_XNSeW9jok   Ali Abdaal   \n",
       "...           ...          ...   \n",
       "1073  FEUIxnnW5-w   Ali Abdaal   \n",
       "1074  d2sMEE1NrFc   Ali Abdaal   \n",
       "1075  7APfFjfnNBc   Ali Abdaal   \n",
       "1076  XJHM0fEH3ss   Ali Abdaal   \n",
       "1077  LQ3Mu8A7gjY   Ali Abdaal   \n",
       "\n",
       "                                                  title  \\\n",
       "0     I¬†Tried AI as¬†a¬†Life Coach for 365 Days - Here...   \n",
       "1            Getting Rich is a Game. Here‚Äôs How to Win.   \n",
       "2     Here's How to Get to the Top 1% (Discipline Is...   \n",
       "3     17 Cheap Purchases That Actually Improved My Life   \n",
       "4                                       How To Get Rich   \n",
       "...                                                 ...   \n",
       "1073  When I Was Your Man (Bruno Mars) - Duranka Per...   \n",
       "1074  Thinking Out Loud (Ed Sheeran) - Ali Abdaal & ...   \n",
       "1075  Payphone (Maroon 5) - Katherine Macfarland & A...   \n",
       "1076  Rolling in the Deep (Adele) - Duranka Perera &...   \n",
       "1077           All of Me (John Legend) - Duranka Perera   \n",
       "\n",
       "                                            description  \\\n",
       "0     #ad Going to therapy is a sign of strength, no...   \n",
       "1     Sign up and download Grammarly for FREE. If yo...   \n",
       "2     Check out Spotter to brainstorm better video i...   \n",
       "3     Check out DeleteMe using my link and get 20% o...   \n",
       "4     üíå Join LifeNotes, my weekly email where I shar...   \n",
       "...                                                 ...   \n",
       "1073  Duranka came over to Cambridge last night to w...   \n",
       "1074  Another (quite bassy) cover of one of our favo...   \n",
       "1075  First cover of 2017! We both love this song, s...   \n",
       "1076  Our first 'proper' video! One of our all-time ...   \n",
       "1077  First attempt at playing around with this sort...   \n",
       "\n",
       "                                                   tags  \\\n",
       "0                           ['Ali Abdaal', 'Ali abdal']   \n",
       "1                           ['Ali Abdaal', 'Ali abdal']   \n",
       "2             ['Ali Abdaal', 'Ali abdal', 'Joe Hudson']   \n",
       "3                           ['Ali Abdaal', 'Ali abdal']   \n",
       "4                           ['Ali Abdaal', 'Ali abdal']   \n",
       "...                                                 ...   \n",
       "1073                                                NaN   \n",
       "1074  ['thinking out loud', 'ed sheeran', 'acoustic ...   \n",
       "1075  ['payphone', 'maroon 5', 'acoustic cover', 'co...   \n",
       "1076  ['rolling in the deep', 'adele', 'cover', 'aco...   \n",
       "1077                                                NaN   \n",
       "\n",
       "                    publishedAt  viewCount  likeCount  favouriteCount  \\\n",
       "0     2025-05-31 13:00:44+00:00      29534       1323             NaN   \n",
       "1     2025-05-28 13:30:01+00:00     101554       4581             NaN   \n",
       "2     2025-05-09 16:23:47+00:00     305074      11357             NaN   \n",
       "3     2025-05-03 13:01:19+00:00     191858       5755             NaN   \n",
       "4     2025-04-18 15:00:51+00:00     645630      29902             NaN   \n",
       "...                         ...        ...        ...             ...   \n",
       "1073  2017-01-28 18:14:19+00:00      23920        306             NaN   \n",
       "1074  2017-01-18 07:13:58+00:00       8736        193             NaN   \n",
       "1075  2017-01-04 23:13:51+00:00     103826       2429             NaN   \n",
       "1076  2016-04-05 09:05:29+00:00      72610        857             NaN   \n",
       "1077  2016-03-27 19:30:11+00:00     158771       1844             NaN   \n",
       "\n",
       "      commentCount   duration definition  caption publishDayName  \\\n",
       "0              145   PT36M42S         hd    False       Saturday   \n",
       "1              312   PT43M12S         hd    False      Wednesday   \n",
       "2              609  PT1H9M34S         hd    False         Friday   \n",
       "3              360   PT16M25S         hd    False       Saturday   \n",
       "4             1795    PT28M7S         hd    False         Friday   \n",
       "...            ...        ...        ...      ...            ...   \n",
       "1073            24    PT3M33S         hd    False       Saturday   \n",
       "1074            46    PT3M42S         hd    False      Wednesday   \n",
       "1075           162    PT2M35S         hd    False      Wednesday   \n",
       "1076            48    PT2M29S         hd    False        Tuesday   \n",
       "1077           187    PT1M43S         hd    False         Sunday   \n",
       "\n",
       "      durationSecs  tagCount  \n",
       "0           2202.0         0  \n",
       "1           2592.0         0  \n",
       "2           4174.0         0  \n",
       "3            985.0         0  \n",
       "4           1687.0         0  \n",
       "...            ...       ...  \n",
       "1073         213.0         0  \n",
       "1074         222.0         0  \n",
       "1075         155.0         0  \n",
       "1076         149.0         0  \n",
       "1077         103.0         0  \n",
       "\n",
       "[1078 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\hafss\\OneDrive\\Desktop\\PFA_2A\\youtube\\data\\video_details.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e714c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'publishedAt' is a datetime column\n",
    "# If it's still a string, parse it first:\n",
    "df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc=True, errors='coerce')\n",
    "\n",
    "# Now create the time-based features\n",
    "df['publish_hour'] = df['publishedAt'].dt.hour\n",
    "df['publish_day_of_week'] = df['publishedAt'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['publish_month'] = df['publishedAt'].dt.month\n",
    "\n",
    "# Days since publish\n",
    "df['days_since_publish'] = (pd.Timestamp.now(tz='UTC') - df['publishedAt']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ccc43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ideo_id</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>favouriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>publishDayName</th>\n",
       "      <th>durationSecs</th>\n",
       "      <th>tagCount</th>\n",
       "      <th>publish_hour</th>\n",
       "      <th>publish_day_of_week</th>\n",
       "      <th>publish_month</th>\n",
       "      <th>days_since_publish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id1</td>\n",
       "      <td>Channel A</td>\n",
       "      <td>Video Title 1: How to start</td>\n",
       "      <td>Desc 1 about starting.</td>\n",
       "      <td>['tag1', 'tag2']</td>\n",
       "      <td>2023-01-01 10:00:00+00:00</td>\n",
       "      <td>15000</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>PT10M5S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2</td>\n",
       "      <td>Channel B</td>\n",
       "      <td>Video Title 2 Explained</td>\n",
       "      <td>Desc 2 explains.</td>\n",
       "      <td>['tag3']</td>\n",
       "      <td>2023-01-05 12:30:00+00:00</td>\n",
       "      <td>50000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>PT20M30S</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3</td>\n",
       "      <td>Channel A</td>\n",
       "      <td>Another Video Idea</td>\n",
       "      <td>Desc 3 has ideas.</td>\n",
       "      <td>['tag1', 'tag4']</td>\n",
       "      <td>2023-01-10 15:00:00+00:00</td>\n",
       "      <td>20000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>PT15M0S</td>\n",
       "      <td>sd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id4</td>\n",
       "      <td>Channel C</td>\n",
       "      <td>Quick Tip #1</td>\n",
       "      <td>Short desc for quick tip.</td>\n",
       "      <td>[]</td>\n",
       "      <td>2023-02-01 08:00:00+00:00</td>\n",
       "      <td>5000</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>PT5M10S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id5</td>\n",
       "      <td>Channel B</td>\n",
       "      <td>Long Tutorial on Topic Y</td>\n",
       "      <td>Very long description for tutorial...</td>\n",
       "      <td>['tag5', 'tag6', 'tag7']</td>\n",
       "      <td>2023-02-15 20:00:00+00:00</td>\n",
       "      <td>75000</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>PT45M0S</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id6</td>\n",
       "      <td>Channel A</td>\n",
       "      <td>Short explainer video</td>\n",
       "      <td>Brief explanation.</td>\n",
       "      <td>['tag1']</td>\n",
       "      <td>2023-03-01 11:00:00+00:00</td>\n",
       "      <td>18000</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>PT8M30S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id7</td>\n",
       "      <td>Channel C</td>\n",
       "      <td>Quick Tip #2</td>\n",
       "      <td>Second quick tip description.</td>\n",
       "      <td>['tag4']</td>\n",
       "      <td>2023-03-05 09:00:00+00:00</td>\n",
       "      <td>6000</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>PT4M0S</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id8</td>\n",
       "      <td>Channel B</td>\n",
       "      <td>Video Review</td>\n",
       "      <td>Review of something.</td>\n",
       "      <td>['review']</td>\n",
       "      <td>2023-03-10 14:00:00+00:00</td>\n",
       "      <td>35000</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>PT25M15S</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id9</td>\n",
       "      <td>Channel A</td>\n",
       "      <td>In-depth Guide</td>\n",
       "      <td>Detailed guide.</td>\n",
       "      <td>['guide']</td>\n",
       "      <td>2023-03-15 17:00:00+00:00</td>\n",
       "      <td>90000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>PT50M0S</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id10</td>\n",
       "      <td>Channel C</td>\n",
       "      <td>Problem Solving</td>\n",
       "      <td>Solving a problem step-by-step.</td>\n",
       "      <td>['problem', 'solution']</td>\n",
       "      <td>2023-03-20 13:00:00+00:00</td>\n",
       "      <td>12000</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>PT12M0S</td>\n",
       "      <td>sd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ideo_id channelTitle                        title  \\\n",
       "0     id1    Channel A  Video Title 1: How to start   \n",
       "1     id2    Channel B      Video Title 2 Explained   \n",
       "2     id3    Channel A           Another Video Idea   \n",
       "3     id4    Channel C                 Quick Tip #1   \n",
       "4     id5    Channel B     Long Tutorial on Topic Y   \n",
       "5     id6    Channel A        Short explainer video   \n",
       "6     id7    Channel C                 Quick Tip #2   \n",
       "7     id8    Channel B                 Video Review   \n",
       "8     id9    Channel A               In-depth Guide   \n",
       "9    id10    Channel C              Problem Solving   \n",
       "\n",
       "                             description                      tags  \\\n",
       "0                 Desc 1 about starting.          ['tag1', 'tag2']   \n",
       "1                       Desc 2 explains.                  ['tag3']   \n",
       "2                      Desc 3 has ideas.          ['tag1', 'tag4']   \n",
       "3              Short desc for quick tip.                        []   \n",
       "4  Very long description for tutorial...  ['tag5', 'tag6', 'tag7']   \n",
       "5                     Brief explanation.                  ['tag1']   \n",
       "6          Second quick tip description.                  ['tag4']   \n",
       "7                   Review of something.                ['review']   \n",
       "8                        Detailed guide.                 ['guide']   \n",
       "9        Solving a problem step-by-step.   ['problem', 'solution']   \n",
       "\n",
       "                publishedAt  viewCount  likeCount  favouriteCount  \\\n",
       "0 2023-01-01 10:00:00+00:00      15000        800               0   \n",
       "1 2023-01-05 12:30:00+00:00      50000       3000               0   \n",
       "2 2023-01-10 15:00:00+00:00      20000       1000               0   \n",
       "3 2023-02-01 08:00:00+00:00       5000        200               0   \n",
       "4 2023-02-15 20:00:00+00:00      75000       4000               0   \n",
       "5 2023-03-01 11:00:00+00:00      18000        900               0   \n",
       "6 2023-03-05 09:00:00+00:00       6000        250               0   \n",
       "7 2023-03-10 14:00:00+00:00      35000       1800               0   \n",
       "8 2023-03-15 17:00:00+00:00      90000       5000               0   \n",
       "9 2023-03-20 13:00:00+00:00      12000        600               0   \n",
       "\n",
       "   commentCount  duration definition  caption publishDayName durationSecs  \\\n",
       "0            50   PT10M5S         hd    False           None         None   \n",
       "1           150  PT20M30S         hd     True           None         None   \n",
       "2            75   PT15M0S         sd    False           None         None   \n",
       "3            10   PT5M10S         hd    False           None         None   \n",
       "4           250   PT45M0S         hd     True           None         None   \n",
       "5            60   PT8M30S         hd    False           None         None   \n",
       "6            15    PT4M0S         hd     True           None         None   \n",
       "7           100  PT25M15S         hd    False           None         None   \n",
       "8           300   PT50M0S         hd     True           None         None   \n",
       "9            40   PT12M0S         sd    False           None         None   \n",
       "\n",
       "  tagCount  publish_hour  publish_day_of_week  publish_month  \\\n",
       "0     None            10                    6              1   \n",
       "1     None            12                    3              1   \n",
       "2     None            15                    1              1   \n",
       "3     None             8                    2              2   \n",
       "4     None            20                    2              2   \n",
       "5     None            11                    2              3   \n",
       "6     None             9                    6              3   \n",
       "7     None            14                    4              3   \n",
       "8     None            17                    2              3   \n",
       "9     None            13                    0              3   \n",
       "\n",
       "   days_since_publish  \n",
       "0                 881  \n",
       "1                 877  \n",
       "2                 872  \n",
       "3                 850  \n",
       "4                 836  \n",
       "5                 822  \n",
       "6                 818  \n",
       "7                 813  \n",
       "8                 808  \n",
       "9                 803  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45af62b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'title', 'description', 'tags', 'publishedAt', 'viewCount',\n",
       "       'likeCount', 'commentCount', 'duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5d4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
